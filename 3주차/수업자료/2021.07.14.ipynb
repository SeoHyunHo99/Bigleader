{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fd0f22",
   "metadata": {},
   "source": [
    "# Chap 6. 다양한 SNS리뷰 정보 수집하기_인스타그램_유투브 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5379960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서\n",
      "진\n",
      "수\n",
      " \n",
      "꽃\n",
      "미\n",
      "남\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "txt = '서진수 꽃미남'\n",
    "for i in txt:\n",
    "    print(i)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19a2abcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   이 크롤러는 인스타그램의 해시태그 정보를 수집합니다\n",
      "   본 제품은 서진수가 교육용으로 특별 제작했으며 \n",
      "   용도외의 사용으로 저작권을 침해하는 행위는 불법입니다\n",
      "   본 제품에 대한 문의는 seojinsu@gmail.com 으로 보내주세요~^^\n",
      "======================================================================\n",
      "1.인스타그램의 ID를 입력하세요: 01095937708\n",
      "2.인스타그램의 비밀번호를 입력하세요: gusgh202!!\n",
      "3.검색할 해쉬태그를 입력하세요(예: 강남맛집): 강남맛집\n",
      "4.크롤링 할 건수는 몇건입니까?: 20\n",
      "5.파일이 저장될 경로만 쓰세요(기본경로 : c:\\temp\\ ) : c:\\temp\\\n",
      "\n",
      "\n",
      "요청하신 데이터를 추출중이오니 잠시만 기다려 주세요~~~~^^\n",
      "\n",
      "\n",
      "요청하신 데이터를 수집중이니 잠시만 기다려 주세요~^^\n",
      "요청하신 데이터를 수집중이니 잠시만 더 기다려 주세요~^^\n",
      "#카와카츠오토코\n",
      "#합정맛집\n",
      "#홍대맛집\n",
      "#연남동맛집\n",
      "#상수맛집\n",
      "#망원동맛집\n",
      "#서울맛집\n",
      "#코밥심_여의도\n",
      "#여의도의축복\n",
      "#아루히\n",
      "#아루히스시\n",
      "#진주집\n",
      "#피양옥여의도점\n",
      "#강남스테이크\n",
      "#서울맛집\n",
      "#서울맛집추천\n",
      "#서울맛집투어\n",
      "#서울맛집베스트\n",
      "#강남맛집\n",
      "#강남맛집추천\n",
      "#강남데이트\n",
      "#강남데이트코스\n",
      "#강남데이트코스추천\n",
      "#강남레스토랑\n",
      "#강남역맛집\n",
      "#강남엿맛집베스트10\n",
      "#일번지통닭\n",
      "#일번지통닭박박사\n",
      "#치킨박박사\n",
      "#통닭박박사\n",
      "#강남맛집\n",
      "#압구정맛집\n",
      "#압구정로데오맛집\n",
      "#닭맛집\n",
      "#인생닭집\n",
      "#치킨맛집\n",
      "#인생맛집\n",
      "#인생맛집박박사\n",
      "#맛집추천박박사\n",
      "#맛스타그램\n",
      "#먹스타그램\n",
      "#닭스타그램\n",
      "#Foodie\n",
      "#Food\n",
      "#Yummy\n",
      "#Eat\n",
      "#Chicken\n",
      "#만수동박박사_압구정\n",
      "#만수동박박사_닭\n",
      "#만수동박박사_치킨\n",
      "#만수동박박사\n",
      "#쿠쿡\n",
      "#CooCook\n",
      "#웅슐랭\n",
      "#강남맛집\n",
      "#강남역맛집\n",
      "#강남가볼만한곳\n",
      "#강남역파스타\n",
      "#강남핫플\n",
      "#강남술집\n",
      "#청담맛집\n",
      "#양재맛집\n",
      "#논현맛집\n",
      "#맛스타그램\n",
      "#맛집\n",
      "#인스타광고\n",
      "#강남맛집\n",
      "#거자필반\n",
      "#삼성역맛집\n",
      "#삼성역\n",
      "#삼성역맛집추천\n",
      "#삼성맛집\n",
      "#삼성맛집추천\n",
      "#강남맛집\n",
      "#강남맛집추천\n",
      "#강남맛집베스트\n",
      "#강남맛집탐방\n",
      "#국밥\n",
      "#국밥맛집\n",
      "#국밥충\n",
      "#곰탕\n",
      "#곰탕맛집\n",
      "#갈비탕\n",
      "#갈비탕맛집\n",
      "#kfood\n",
      "#koreanfood\n",
      "#혜장국\n",
      "#수육\n",
      "#육개장\n",
      "#신논현맛집\n",
      "#스겜\n",
      "#신논현역맛집\n",
      "#강남역맛집\n",
      "#서초동맛집\n",
      "#강남맛집\n",
      "#서초맛집\n",
      "#서초구맛집\n",
      "#그레이터_서초\n",
      "#강남맛집\n",
      "#강남역맛집\n",
      "#강남역핫플\n",
      "#강남역가볼만한곳\n",
      "#강남역파스타\n",
      "#강남역술집\n",
      "#일상\n",
      "#맛스타그램\n",
      "#맛집\n",
      "#강남맛집\n",
      "#막국수\n",
      "#송림면옥\n",
      "#비빔막국수\n",
      "#물막국수\n",
      "#백숙찜닭\n",
      "#죽\n",
      "#도가니닭곰탕\n",
      "#김치맛집\n",
      "#초계탕\n",
      "#갈비찜\n",
      "#서울\n",
      "#서울맛집\n",
      "#강남\n",
      "#강남맛집\n",
      "#역삼동\n",
      "#역삼동맛집\n",
      "#역삼역맛집\n",
      "#먜슐랭\n",
      "#먜슐랭_강남\n",
      "#먜슐랭_역삼동\n",
      "#문썬과함께\n",
      "#드자벨\n",
      "#드자벨네일\n",
      "#강남역\n",
      "#강남왁싱\n",
      "#강남역네일\n",
      "#네일추천\n",
      "#글리터네일\n",
      "#강남역젤네일\n",
      "#젤아트\n",
      "#젤네일\n",
      "#강남네일샵\n",
      "#젤아트\n",
      "#강남맛집\n",
      "#포인트아트\n",
      "#강남속눈썹\n",
      "#속눈썹연장\n",
      "#웨딩네일아트\n",
      "#nailart\n",
      "#속눈썹펌\n",
      "#노란색\n",
      "#그라데이션\n",
      "#네일인스타그램\n",
      "#네일디자인\n",
      "#강남속눈썹펌\n",
      "#강남역속눈썹\n",
      "#강남속눈썹연장\n",
      "#포인트네일\n",
      "#강남역네일아트\n",
      "#젤네일아트\n",
      "#스와로브스키\n",
      "#네일그램\n",
      "#강남맛집\n",
      "#논현맛집\n",
      "#신논현맛집\n",
      "#팔덕식당논현점\n",
      "#비숑프리제\n",
      "#미니비숑\n",
      "#반려견\n",
      "#독스타그램\n",
      "#bichonfrise\n",
      "#멍스타그램\n",
      "#강력본드걸\n",
      "#사랑해❤\n",
      "#子犬\n",
      "#ビション\n",
      "#小狗\n",
      "#こいぬ\n",
      "#犬\n",
      "#dog\n",
      "#puppy\n",
      "#bichon\n",
      "#펫스타그램\n",
      "#instardog\n",
      "#instarpet\n",
      "#instarbichon\n",
      "#bichonsofinstagram\n",
      "#bichon\n",
      "#daily\n",
      "#소점\n",
      "#오코노미야끼\n",
      "#다다샵7597\n",
      "#샤넬가방\n",
      "#사이즈11\n",
      "#맛집\n",
      "#이쁜카페\n",
      "#감성카페\n",
      "#맛집추천\n",
      "#맛집스타그램\n",
      "#소통\n",
      "#강남맛집\n",
      "#디저트카페\n",
      "#카페투어\n",
      "#브런치카페\n",
      "#패피템\n",
      "#데일리코디\n",
      "#루이비통\n",
      "#펜디\n",
      "#프라다\n",
      "#버버리\n",
      "#샤넬\n",
      "#구찌\n",
      "#디올\n",
      "#食事\n",
      "#ローリズプライムリブ\n",
      "#ステーキ\n",
      "#ステーキハウス\n",
      "#洋食\n",
      "#おしゃれなお店\n",
      "#素敵なお店\n",
      "#로리스더프라임립\n",
      "#스테이크\n",
      "#스테이크맛집\n",
      "#스테이크하우스\n",
      "#레스토랑추천\n",
      "#강남역\n",
      "#프라임립\n",
      "#샴페인\n",
      "#dinner\n",
      "#restaurant\n",
      "#champagne\n",
      "#champagnelover\n",
      "#lawrystheprimerib\n",
      "#primerib\n",
      "#夕食\n",
      "#食事\n",
      "#グルメ\n",
      "#お肉\n",
      "#강남역맛집\n",
      "#스테이크맛집\n",
      "#데이트맛집\n",
      "#기념일레스토랑\n",
      "#강남맛집\n",
      "#샴페인\n",
      "#シャンパン\n",
      "#シャンパン大好き\n",
      "#다다샵7597\n",
      "#샤넬가방\n",
      "#사이즈11\n",
      "#맛집\n",
      "#이쁜카페\n",
      "#감성카페\n",
      "#맛집추천\n",
      "#맛집스타그램\n",
      "#소통\n",
      "#강남맛집\n",
      "#디저트카페\n",
      "#카페투어\n",
      "#브런치카페\n",
      "#패피템\n",
      "#데일리코디\n",
      "#루이비통\n",
      "#펜디\n",
      "#프라다\n",
      "#버버리\n",
      "#샤넬\n",
      "#구찌\n",
      "#디올\n",
      "#애월카페\n",
      "#서면맛집\n",
      "#강남맛집\n",
      "#홍대맛집\n",
      "#제주카페\n",
      "#화장품스타드램\n",
      "#새콤\n",
      "#핵존맛\n",
      "#화장품추천\n",
      "#저녁반찬\n",
      "#먹고또먹고\n",
      "#동성로맛집\n",
      "#순살치킨\n",
      "#라엔뷰티\n",
      "#블랙마스카라에센스\n",
      "====================================================================================================\n",
      "총 소요시간: 109.9 초\n",
      "총 저장 건수: 20 건 \n",
      "파일 저장 경로: c:\\temp\\2021-07-14-14-13-46-인스타그램-강남맛집\\2021-07-14-14-13-46-강남맛집.txt\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# 인스타 그램의 해쉬태그 수집하기 - by 서진수\n",
    "##########################################################################\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import unicodedata   # 인스타그램의 해시태그 수집 중 자음/모음 분리현상 방지용 모듈\n",
    "\n",
    "#Step 2. 사용자에게 필요한 정보들을를 입력 받습니다.\n",
    "print(\"=\" *70)\n",
    "print(\"   이 크롤러는 인스타그램의 해시태그 정보를 수집합니다\")\n",
    "print(\"   본 제품은 서진수가 교육용으로 특별 제작했으며 \")\n",
    "print(\"   용도외의 사용으로 저작권을 침해하는 행위는 불법입니다\")\n",
    "print(\"   본 제품에 대한 문의는 seojinsu@gmail.com 으로 보내주세요~^^\")\n",
    "print(\"=\" *70)\n",
    "\n",
    "v_id = input(\"1.인스타그램의 ID를 입력하세요: \")\n",
    "v_passwd = input(\"2.인스타그램의 비밀번호를 입력하세요: \")\n",
    "query_txt = input(\"3.검색할 해쉬태그를 입력하세요(예: 강남맛집): \")\n",
    "cnt = int(input('4.크롤링 할 건수는 몇건입니까?: '))\n",
    "real_cnt = math.ceil(cnt / 10)\n",
    "\n",
    "f_dir=input('5.파일이 저장될 경로만 쓰세요(기본경로 : c:\\\\temp\\\\ ) : ')\n",
    "if f_dir =='' :\n",
    "    f_dir = \"c:\\\\temp\\\\\"\n",
    "\n",
    "#Step 3. 결과를 저장할 폴더명과 파일명을 설정하고 폴더를 생성합니다.\n",
    "s_time = time.time( )\n",
    "query_txt2 = '인스타그램'\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "# Step 4. 인스타그램 자동 로그인 하기\n",
    "chrome_path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "\n",
    "driver.get(\"https://www.instagram.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(random.randrange(1,5))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"요청하신 데이터를 추출중이오니 잠시만 기다려 주세요~~~~^^\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#ID와 비번 입력후 로그인하기\n",
    "eid = driver.find_element_by_name('username')\n",
    "for a in v_id :\n",
    "        eid.send_keys(a)\n",
    "        time.sleep(0.3)\n",
    "epwd = driver.find_element_by_name('password')\n",
    "for b in v_passwd :\n",
    "        epwd.send_keys(b)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"loginForm\"]/div/div[3]/button/div').click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5. 검색할 해쉬태그 입력하기\n",
    "#로그인 정보 나중에 저장하기\n",
    "driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/div/div/button').click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_xpath('/html/body/div[4]/div/div/div/div[3]/button[2]').click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# 검색할 키워드 입력하기\n",
    "element = driver.find_element_by_xpath('''//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/input''')\n",
    "\n",
    "for c in query_txt :\n",
    "    element.send_keys(c)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/div[3]/div/div[2]/div/div[1]/a').click()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 자동 스크롤다운 함수\n",
    "def scroll_down(driver):\n",
    "  driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "  time.sleep(10)\n",
    "\n",
    "print('요청하신 데이터를 수집중이니 잠시만 기다려 주세요~^^')\n",
    "item=[]     # 인스타그램 URL 주소 저장할 리스트\n",
    "item2=[]\n",
    "    \n",
    "a = 1\n",
    "while (a <= real_cnt):\n",
    "    scroll_down(driver) \n",
    "\n",
    "    # Step 6. 전체 게시물의 원본 URL 추출하기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    all = soup.find('article','KC1QD').find_all('a')\n",
    "\n",
    "    for i in all:    \n",
    "        url = i['href']\n",
    "        item.append(url)          \n",
    "        item2 = pd.Series(item).drop_duplicates()\n",
    "    \n",
    "        if len(item2) >= cnt :\n",
    "            break\n",
    "    a += 1\n",
    "    print('요청하신 데이터를 수집중이니 잠시만 더 기다려 주세요~^^')\n",
    "    \n",
    "    if len(item2) >= cnt :\n",
    "        break\n",
    "   \n",
    "# 추출된 URL 사용하여 전체 URL 완성하기\n",
    "full_url=[]\n",
    "url_cnt = 0\n",
    "for x in range(0,len(item2)) :\n",
    "    url = 'https://www.instagram.com' +item[x]\n",
    "    full_url.append(url)\n",
    "    url_cnt += 1\n",
    "    \n",
    "    if url_cnt > cnt:\n",
    "        break\n",
    "\n",
    "# 아래 for 반복문은 수집된 URL 주소를 확인하는 코드임        \n",
    "#for c in range(1,len(full_url)+1) :\n",
    "#    print(c,':',full_url[c-1])\n",
    "\n",
    "#Step 7. 각 페이지별로 그림과 해쉬태그를 수집하기\n",
    "count = 1        # 추출 데이터 건수 세기\n",
    "hash_txt = []    # 해쉬 태그 저장 \n",
    "\n",
    "# 비트맵 이미지 아이콘을 위한 대체 딕셔너리를 만든다\n",
    "import sys\n",
    "bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "count = 0\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt2+'-'+query_txt)\n",
    "f_name=f_dir+s+'-'+query_txt2+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "\n",
    "for c in range(0,len(full_url)) :\n",
    "    driver.get(full_url[c])\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    f = open(f_name, 'a',encoding='UTF-8')\n",
    "\n",
    "    # 해당 페이지의 해시태그 수집\n",
    "    tags = soup.find('div','EtaWk')\n",
    "\n",
    "    try :\n",
    "        tags_1 = tags.find_all('a')\n",
    "    except :\n",
    "        pass\n",
    "    else :\n",
    "        for d in range(0, len(tags_1)) :\n",
    "            tags = tags_1[d].get_text()\n",
    "            tags_11 = tags.translate(bmp_map)\n",
    "            tags_2 = unicodedata.normalize('NFC', tags_11)\n",
    "\n",
    "            for i in tags_2 :\n",
    "                if i[0:1]=='#' :\n",
    "                    hash_txt.append(tags_2)\n",
    "                    print(tags_2)\n",
    "                    f.write(\"\\n\" + str(tags_2))\n",
    "    f.close()\n",
    "    count += 1\n",
    "    \n",
    "#Step 7. 요약 정보 출력하기    \n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"=\" *100)\n",
    "print(\"총 소요시간: %s 초\" %round(t_time,1))\n",
    "print(\"총 저장 건수: %s 건 \" %count)\n",
    "print(\"파일 저장 경로: %s\" %f_name)\n",
    "print(\"=\" *100)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17da1af",
   "metadata": {},
   "source": [
    "# Chap 6-1. 연습문제\n",
    "인스타그램에 자동으로 로그인 한 후 “강남맛집추천”으로 검색하여 나오는 게시물 중 50개를\n",
    "추출하여 사진과 해시태그를 수집하여 사진은 사진 폴더에 별도로 저장하고(사진파일의 이름은\n",
    "1.jpg 부터 50.jpg) 해시태그는 txt파일 형식으로 저장하는 웹 크롤러를 저장하세요. (단 게시물의\n",
    "사진이 여러 장 일 경우 대표사진 1 장만 수집하여 저장하면 됩니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a678a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 인스타 그램의 해쉬태그 수집하기 - by 서진수\n",
    "##########################################################################\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import unicodedata   # 인스타그램의 해시태그 수집 중 자음/모음 분리현상 방지용 모듈\n",
    "\n",
    "#Step 2. 사용자에게 필요한 정보들을를 입력 받습니다.\n",
    "print(\"=\" *70)\n",
    "print(\"   이 크롤러는 인스타그램의 해시태그 정보를 수집합니다\")\n",
    "print(\"   본 제품은 서진수가 교육용으로 특별 제작했으며 \")\n",
    "print(\"   용도외의 사용으로 저작권을 침해하는 행위는 불법입니다\")\n",
    "print(\"   본 제품에 대한 문의는 seojinsu@gmail.com 으로 보내주세요~^^\")\n",
    "print(\"=\" *70)\n",
    "\n",
    "v_id = input(\"1.인스타그램의 ID를 입력하세요: \")\n",
    "v_passwd = input(\"2.인스타그램의 비밀번호를 입력하세요: \")\n",
    "query_txt = input(\"3.검색할 해쉬태그를 입력하세요(예: 강남맛집): \")\n",
    "cnt = int(input('4.크롤링 할 건수는 몇건입니까?: '))\n",
    "real_cnt = math.ceil(cnt / 10)\n",
    "\n",
    "f_dir=input('5.파일이 저장될 경로만 쓰세요(기본경로 : c:\\\\temp\\\\ ) : ')\n",
    "if f_dir =='' :\n",
    "    f_dir = \"c:\\\\temp\\\\\"\n",
    "\n",
    "#Step 3. 결과를 저장할 폴더명과 파일명을 설정하고 폴더를 생성합니다.\n",
    "s_time = time.time( )\n",
    "query_txt2 = '인스타그램'\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "# Step 4. 인스타그램 자동 로그인 하기\n",
    "chrome_path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "\n",
    "driver.get(\"https://www.instagram.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(random.randrange(1,5))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"요청하신 데이터를 추출중이오니 잠시만 기다려 주세요~~~~^^\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#ID와 비번 입력후 로그인하기\n",
    "eid = driver.find_element_by_name('username')\n",
    "for a in v_id :\n",
    "        eid.send_keys(a)\n",
    "        time.sleep(0.3)\n",
    "epwd = driver.find_element_by_name('password')\n",
    "for b in v_passwd :\n",
    "        epwd.send_keys(b)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"loginForm\"]/div/div[3]/button/div').click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5. 검색할 해쉬태그 입력하기\n",
    "#로그인 정보 나중에 저장하기\n",
    "driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/div/div/button').click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_xpath('/html/body/div[4]/div/div/div/div[3]/button[2]').click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# 검색할 키워드 입력하기\n",
    "element = driver.find_element_by_xpath('''//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/input''')\n",
    "\n",
    "for c in query_txt :\n",
    "    element.send_keys(c)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/div[3]/div/div[2]/div/div[1]/a').click()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 자동 스크롤다운 함수\n",
    "def scroll_down(driver):\n",
    "  driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "  time.sleep(10)\n",
    "\n",
    "print('요청하신 데이터를 수집중이니 잠시만 기다려 주세요~^^')\n",
    "item=[]     # 인스타그램 URL 주소 저장할 리스트\n",
    "item2=[]\n",
    "    \n",
    "a = 1\n",
    "while (a <= real_cnt):\n",
    "    scroll_down(driver) \n",
    "\n",
    "    # Step 6. 전체 게시물의 원본 URL 추출하기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    all = soup.find('article','KC1QD').find_all('a')\n",
    "\n",
    "    for i in all:    \n",
    "        url = i['href']\n",
    "        item.append(url)          \n",
    "        item2 = pd.Series(item).drop_duplicates()\n",
    "    \n",
    "        if len(item2) >= cnt :\n",
    "            break\n",
    "    a += 1\n",
    "    print('요청하신 데이터를 수집중이니 잠시만 더 기다려 주세요~^^')\n",
    "    \n",
    "    if len(item2) >= cnt :\n",
    "        break\n",
    "   \n",
    "# 추출된 URL 사용하여 전체 URL 완성하기\n",
    "full_url=[]\n",
    "url_cnt = 0\n",
    "for x in range(0,len(item2)) :\n",
    "    url = 'https://www.instagram.com' +item[x]\n",
    "    full_url.append(url)\n",
    "    url_cnt += 1\n",
    "    \n",
    "    if url_cnt > cnt:\n",
    "        break\n",
    "\n",
    "# 아래 for 반복문은 수집된 URL 주소를 확인하는 코드임        \n",
    "#for c in range(1,len(full_url)+1) :\n",
    "#    print(c,':',full_url[c-1])\n",
    "\n",
    "#Step 7. 각 페이지별로 그림과 해쉬태그를 수집하기\n",
    "count = 1        # 추출 데이터 건수 세기\n",
    "hash_txt = []    # 해쉬 태그 저장 \n",
    "\n",
    "# 비트맵 이미지 아이콘을 위한 대체 딕셔너리를 만든다\n",
    "import sys\n",
    "bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "count = 0\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt2+'-'+query_txt)\n",
    "f_name=f_dir+s+'-'+query_txt2+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "\n",
    "for c in range(0,len(full_url)) :\n",
    "    driver.get(full_url[c])\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    f = open(f_name, 'a',encoding='UTF-8')\n",
    "\n",
    "    # 해당 페이지의 해시태그 수집\n",
    "    tags = soup.find('div','EtaWk')\n",
    "\n",
    "    try :\n",
    "        tags_1 = tags.find_all('a')\n",
    "    except :\n",
    "        pass\n",
    "    else :\n",
    "        for d in range(0, len(tags_1)) :\n",
    "            tags = tags_1[d].get_text()\n",
    "            tags_11 = tags.translate(bmp_map)\n",
    "            tags_2 = unicodedata.normalize('NFC', tags_11)\n",
    "\n",
    "            for i in tags_2 :\n",
    "                if i[0:1]=='#' :\n",
    "                    hash_txt.append(tags_2)\n",
    "                    print(tags_2)\n",
    "                    f.write(\"\\n\" + str(tags_2))\n",
    "    f.close()\n",
    "    count += 1\n",
    "    \n",
    "#Step 7. 요약 정보 출력하기    \n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"=\" *100)\n",
    "print(\"총 소요시간: %s 초\" %round(t_time,1))\n",
    "print(\"총 저장 건수: %s 건 \" %count)\n",
    "print(\"파일 저장 경로: %s\" %f_name)\n",
    "print(\"=\" *100)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd3fc879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사진을 저장할 폴더를 지정하세요(예: c:\\data\\) : c:\\data\\\n",
      "1 번째 이미지 저장중입니다=======\n",
      "2 번째 이미지 저장중입니다=======\n",
      "3 번째 이미지 저장중입니다=======\n",
      "4 번째 이미지 저장중입니다=======\n",
      "5 번째 이미지 저장중입니다=======\n",
      "6 번째 이미지 저장중입니다=======\n",
      "7 번째 이미지 저장중입니다=======\n",
      "8 번째 이미지 저장중입니다=======\n",
      "9 번째 이미지 저장중입니다=======\n",
      "10 번째 이미지 저장중입니다=======\n",
      "11 번째 이미지 저장중입니다=======\n",
      "12 번째 이미지 저장중입니다=======\n",
      "13 번째 이미지 저장중입니다=======\n",
      "14 번째 이미지 저장중입니다=======\n",
      "======================================================================\n",
      "총 소요시간은 20.6 초 입니다 \n",
      "총 저장 건수는 14 건 입니다 \n",
      "파일 저장 경로: c:\\data\\2021-07-14-14-23-08-사진저장 입니다\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#Chap 7-1. 이미지 다운로드용 웹크롤러 만들기\n",
    "# Step 1. 필요한 모듈과 라이브러리를 로딩하고 검색어를 입력 받습니다\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "f_dir = input('사진을 저장할 폴더를 지정하세요(예: c:\\data\\) : ')\n",
    "query_txt ='사진저장'\n",
    "\n",
    "#Step 2. 파일을 저장할 폴더를 생성합니다\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.chdir(f_dir)\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "f_result_dir = f_dir+s+'-'+query_txt\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "s_time = time.time( )      # 크롤링 시작 시간을 위한 타임 스탬프를 찍습니다\n",
    "\n",
    "driver.get(\"https://korean.visitkorea.or.kr/detail/rem_detail.html?cotid=be3db10c-b642-409c-81cc-c4cdecb5bd8b&temp=\")\n",
    "time.sleep(2)  # 페이지가 모두 열릴 때 까지 2초 기다립니다.\n",
    "\n",
    "# 학습목표 1: 자동 스크롤다운 함수 만들기\n",
    "# Step 4.  스크롤다운 함수를 생성한 후 실행합니다.\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "scroll_down(driver) \n",
    "    \n",
    "\n",
    "# 본문의 사진 정보를 가져와서 지정된 폴더에 저장하기    \n",
    "# Step 5. 이미지 추출하여 저장하기 \n",
    "\n",
    "file_no = 0                                \n",
    "count = 1\n",
    "img_src2=[]\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img_src = soup.find('div','box_txtPhoto').find_all('img')\n",
    "\n",
    "for i in img_src :\n",
    "        img_src1=i['src']\n",
    "        img_src2.append(img_src1)\n",
    "        count += 1\n",
    "\n",
    "for i in range(0,len(img_src2)) :\n",
    "        try :\n",
    "                urllib.request.urlretrieve(img_src2[i],str(file_no)+'.jpg')\n",
    "        except :\n",
    "                continue        \n",
    "        file_no += 1                \n",
    "        time.sleep(0.5)      \n",
    "        print(\"%s 번째 이미지 저장중입니다=======\" %file_no)\n",
    "       \n",
    "# Step 6. 요약 정보를 출력합니다                \n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "store_cnt = file_no -1\n",
    "\n",
    "print(\"=\" *70)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"총 저장 건수는 %s 건 입니다 \" %file_no)\n",
    "print(\"파일 저장 경로: %s 입니다\" %f_result_dir)\n",
    "print(\"=\" *70)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e5380",
   "metadata": {},
   "source": [
    "# Chap 7. 다양한 이미지 수집 크롤러 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4734cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사진을 저장할 폴더를 지정하세요(예: c:\\data\\) : c:\\data\\\n",
      "1 번째 이미지 저장중입니다=======\n",
      "2 번째 이미지 저장중입니다=======\n",
      "3 번째 이미지 저장중입니다=======\n",
      "4 번째 이미지 저장중입니다=======\n",
      "5 번째 이미지 저장중입니다=======\n",
      "6 번째 이미지 저장중입니다=======\n",
      "7 번째 이미지 저장중입니다=======\n",
      "8 번째 이미지 저장중입니다=======\n",
      "9 번째 이미지 저장중입니다=======\n",
      "10 번째 이미지 저장중입니다=======\n",
      "11 번째 이미지 저장중입니다=======\n",
      "12 번째 이미지 저장중입니다=======\n",
      "13 번째 이미지 저장중입니다=======\n",
      "14 번째 이미지 저장중입니다=======\n",
      "======================================================================\n",
      "총 소요시간은 53.1 초 입니다 \n",
      "총 저장 건수는 14 건 입니다 \n",
      "파일 저장 경로: c:\\data\\2021-07-14-13-58-03-사진저장 입니다\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#Chap 7-1. 이미지 다운로드용 웹크롤러 만들기\n",
    "# Step 1. 필요한 모듈과 라이브러리를 로딩하고 검색어를 입력 받습니다\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "f_dir = input('사진을 저장할 폴더를 지정하세요(예: c:\\data\\) : ')\n",
    "query_txt ='사진저장'\n",
    "\n",
    "#Step 2. 파일을 저장할 폴더를 생성합니다\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.chdir(f_dir)\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "f_result_dir = f_dir+s+'-'+query_txt\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "s_time = time.time( )      # 크롤링 시작 시간을 위한 타임 스탬프를 찍습니다\n",
    "\n",
    "driver.get(\"https://korean.visitkorea.or.kr/detail/rem_detail.html?cotid=be3db10c-b642-409c-81cc-c4cdecb5bd8b&temp=\")\n",
    "time.sleep(2)  # 페이지가 모두 열릴 때 까지 2초 기다립니다.\n",
    "\n",
    "# 학습목표 1: 자동 스크롤다운 함수 만들기\n",
    "# Step 4.  스크롤다운 함수를 생성한 후 실행합니다.\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "scroll_down(driver) \n",
    "    \n",
    "\n",
    "# 본문의 사진 정보를 가져와서 지정된 폴더에 저장하기    \n",
    "# Step 5. 이미지 추출하여 저장하기 \n",
    "\n",
    "file_no = 0                                \n",
    "count = 1\n",
    "img_src2=[]\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img_src = soup.find('div','box_txtPhoto').find_all('img')\n",
    "\n",
    "for i in img_src :\n",
    "        img_src1=i['src']\n",
    "        img_src2.append(img_src1)\n",
    "        count += 1\n",
    "\n",
    "for i in range(0,len(img_src2)) :\n",
    "        try :\n",
    "                urllib.request.urlretrieve(img_src2[i],str(file_no)+'.jpg')\n",
    "        except :\n",
    "                continue        \n",
    "        file_no += 1                \n",
    "        time.sleep(0.5)      \n",
    "        print(\"%s 번째 이미지 저장중입니다=======\" %file_no)\n",
    "       \n",
    "# Step 6. 요약 정보를 출력합니다                \n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "store_cnt = file_no -1\n",
    "\n",
    "print(\"=\" *70)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"총 저장 건수는 %s 건 입니다 \" %file_no)\n",
    "print(\"파일 저장 경로: %s 입니다\" %f_result_dir)\n",
    "print(\"=\" *70)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "943ab892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " 쿠팡 사이트의 식품 카테고리 Best Seller 상품 정보 추출하기 \n",
      "================================================================================\n",
      "1.크롤링 할 건수는 몇건입니까?: 10\n",
      "2.파일을 저장할 폴더명만 쓰세요(기본경로:c:\\temp\\):c:\\temp\\\n",
      "\n",
      "\n",
      "    요청하신 데이터를 수집하고 있으니 잠시만 기다려 주세요~~\n",
      "\n",
      "\n",
      "===== 곧 수집된 결과를 출력합니다 ^^ ===== \n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 1\n",
      "2.제품소개: 바다자리 개조개, 700g, 1개\n",
      "3.판매가격: 15,800\n",
      "4:할인률: 19%\n",
      "5.상품평 수: 2\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 2\n",
      "2.제품소개: 폴바셋 시그니처 블렌드 원두+리유저블 텀블러 증정, 200g\n",
      "3.판매가격: 13,300\n",
      "4:할인률: 12%\n",
      "5.상품평 수: 38\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 3\n",
      "2.제품소개: 제주 삼다수, 2L, 24개\n",
      "3.판매가격: 23,490\n",
      "4:할인률: 40%\n",
      "5.상품평 수: 303272\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 4\n",
      "2.제품소개: 가야산 천년수 무라벨, 2L, 24개\n",
      "3.판매가격: 9,900\n",
      "4:할인률: 0\n",
      "5.상품평 수: 23485\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 5\n",
      "2.제품소개: 상하목장 얼려먹는 아이스크림 밀크, 85ml, 24개\n",
      "3.판매가격: 20,210\n",
      "4:할인률: 7%\n",
      "5.상품평 수: 284\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 6\n",
      "2.제품소개: 탐사수, 500ml, 40개\n",
      "3.판매가격: 8,480\n",
      "4:할인률: 28%\n",
      "5.상품평 수: 629799\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 7\n",
      "2.제품소개: 칼로바이 프로틴 스파클링 단백질 음료 분리유청 WPI 헬스보충제 12개입, 240ml\n",
      "3.판매가격: 15,900\n",
      "4:할인률: 47%\n",
      "5.상품평 수: 1109\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 8\n",
      "2.제품소개: 신선란 미국산 30구, 1700g, 1팩\n",
      "3.판매가격: 6,540\n",
      "4:할인률: 6%\n",
      "5.상품평 수: 16121\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 9\n",
      "2.제품소개: 탐사수, 2L, 24개\n",
      "3.판매가격: 11,690\n",
      "4:할인률: 31%\n",
      "5.상품평 수: 97670\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "1.판매순위: 10\n",
      "2.제품소개: 풀무원 한끼 연두부 + 오리엔탈 유자 소스 6개입, 110g, 1세트\n",
      "3.판매가격: 6,960\n",
      "4:할인률: 11%\n",
      "5.상품평 수: 16946\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=91.0.4472.124)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-8305e6c1f684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_link_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 다음 페이지번호 클릭\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;31m#step 6. csv , xls 형태로 저장하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_link_text\u001b[1;34m(self, link_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_link_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sign In'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \"\"\"\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINK_TEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_link_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=91.0.4472.124)\n"
     ]
    }
   ],
   "source": [
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd    \n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "#Step 2. 사용자에게 검색어 키워드를 입력 받습니다.\n",
    "print(\"=\" *80)\n",
    "print(\" 쿠팡 사이트의 식품 카테고리 Best Seller 상품 정보 추출하기 \")\n",
    "print(\"=\" *80)\n",
    "\n",
    "cnt = int(input('1.크롤링 할 건수는 몇건입니까?: '))\n",
    "page_cnt = math.ceil(cnt/60)\n",
    "\n",
    "f_dir = input(\"2.파일을 저장할 폴더명만 쓰세요(기본경로:c:\\\\temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir = \"c:\\\\temp\\\\\"\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "if cnt > 30 :\n",
    "      print(\"    요청 건수가 많아서 시간이 제법 소요되오니 잠시만 기다려 주세요~~\")\n",
    "else :\n",
    "      print(\"    요청하신 데이터를 수집하고 있으니 잠시만 기다려 주세요~~\")\n",
    "\n",
    "#Step 3.저장될 파일 경로와 이름을 지정합니다\n",
    "sec_name = '식품'\n",
    "query_txt='쿠팡'\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt+'-'+sec_name)\n",
    "os.chdir(f_dir+s+'-'+query_txt+'-'+sec_name)\n",
    "\n",
    "ff_dir=f_dir+s+'-'+query_txt+'-'+sec_name\n",
    "ff_name=f_dir+s+'-'+query_txt+'-'+sec_name+'\\\\'+s+'-'+query_txt+'-'+sec_name+'.txt'\n",
    "fc_name=f_dir+s+'-'+query_txt+'-'+sec_name+'\\\\'+s+'-'+query_txt+'-'+sec_name+'.csv'\n",
    "fx_name=f_dir+s+'-'+query_txt+'-'+sec_name+'\\\\'+s+'-'+query_txt+'-'+sec_name+'.xls'\n",
    "\n",
    "# 제품 이미지 저장용 폴더 생성\n",
    "img_dir = ff_dir+\"\\\\images\"\n",
    "os.makedirs(img_dir)\n",
    "os.chdir(img_dir)\n",
    "    \n",
    "s_time = time.time( )\n",
    "\n",
    "#Step 4. 웹사이트 접속 후 해당 메뉴로 이동합니다.\n",
    "chrome_path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "query_url='https://www.coupang.com/'\n",
    "driver.get(query_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# 쿠키 지우기\n",
    "driver.delete_all_cookies()\n",
    "time.sleep(2)\n",
    "\n",
    "# 분야별 더보기 버튼을 눌러 페이지를 엽니다\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"header\"]/div\"\"\").click( )\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"gnbAnalytics\"]/ul[1]/li[4]/a\"\"\").click( )\n",
    "\n",
    "#Step 5. 내용을 수집합니다\n",
    "print(\"\\n\")\n",
    "print(\"===== 곧 수집된 결과를 출력합니다 ^^ ===== \")\n",
    "print(\"\\n\")\n",
    "\n",
    "ranking2=[]        #제품의 판매순위 저장\n",
    "title2=[]          #제품 정보 저장\n",
    "p_price2=[]        #현재 판매가 저장\n",
    "discount2 = []     #할인율 저장\n",
    "sat_count2=[]      #상품평 수 저장\n",
    "\n",
    "img_src2=[]   # 이미지 URL 저장변수\n",
    "file_no = 0   # 이미지 파일 저장할 때 번호\n",
    "count = 1     # 총 게시물 건수 카운트 변수\n",
    "\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "scroll_down(driver)   #현재화면의 가장 아래로 스크롤다운합니다\n",
    "\n",
    "for x in range(1,page_cnt + 1) :\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    item_result = soup.find('ul','baby-product-list').find_all('li')\n",
    "\n",
    "    for li in item_result :\n",
    "        if cnt < count :\n",
    "            break\n",
    "\n",
    "        # 제품 이미지 다운로드 하기\n",
    "        import urllib.request\n",
    "        import urllib\n",
    "\n",
    "        try :\n",
    "            photo = li.find('dt','image').find('img')['src']\n",
    "        except AttributeError : # 이미지가 없을 경우도 있기 때문 \n",
    "            continue\n",
    "\n",
    "        file_no += 1\n",
    "        full_photo = 'https:' + photo\n",
    "        urllib.request.urlretrieve(full_photo,str(file_no)+'.jpg')\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        #제품 내용 추출하기\n",
    "        f = open(ff_name, 'a',encoding='UTF-8')\n",
    "        f.write(\"-----------------------------------------------------\"+\"\\n\")\n",
    "        print(\"-\" *70)\n",
    "\n",
    "        ranking = count\n",
    "        print(\"1.판매순위:\",ranking)\n",
    "        f.write('1.판매순위:'+ str(ranking) + \"\\n\")\n",
    "\n",
    "        try :\n",
    "            t = li.find('div','name').get_text().replace(\"\\n\",\"\")\n",
    "        except :\n",
    "            title = '제품소개가 없습니다'\n",
    "            print(title.replace(\"\\n\",\"\"))\n",
    "            f.write('2.제품소개:'+ title + \"\\n\")\n",
    "        else :\n",
    "            title = t.replace(\"\\n\",\"\").strip()\n",
    "            print(\"2.제품소개:\", title.replace(\"\\n\",\"\").strip())                  \n",
    "            f.write('2.제품소개:'+ title + \"\\n\")\n",
    "\n",
    "        try :\n",
    "            p_price = li.find('strong','price-value').get_text().replace(\"\\n\",\"\")\n",
    "        except :\n",
    "            p_price = '0'\n",
    "            print(\"3.판매가격:\", p_price.replace(\"\\n\",\"\"))\n",
    "            f.write('3.판매가격:'+ p_price + \"\\n\")\n",
    "        else :\n",
    "            print(\"3.판매가격:\", p_price.replace(\"\\n\",\"\"))\n",
    "            f.write('3.판매가격:'+ p_price + \"\\n\")\n",
    "\n",
    "        try :\n",
    "            discount = li.find('span','discount-percentage').get_text().replace(\"\\n\",\"\")\n",
    "        except  :\n",
    "            discount = '0'\n",
    "            print(\"4:할인률:\", discount)\n",
    "            f.write('4.할인율:'+ discount + \"\\n\")\n",
    "        else :\n",
    "            print(\"4:할인률:\", discount)\n",
    "            f.write('4.할인율:'+ discount + \"\\n\")\n",
    "\n",
    "        try :\n",
    "            sat_count_1 = li.find('span','rating-total-count').get_text()\n",
    "            sat_count_2 = sat_count_1.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "        except  :\n",
    "            sat_count_2='0'\n",
    "            print('5.상품평 수: ',sat_count_2)\n",
    "            f.write('5.상품평 수:'+ sat_count_2 + \"\\n\")\n",
    "        else :\n",
    "            print('5.상품평 수:',sat_count_2)\n",
    "            f.write('5.상품평 수:'+ sat_count_2 + \"\\n\")\n",
    "\n",
    "        print(\"-\" *70)\n",
    "\n",
    "        f.close( )             \n",
    "        time.sleep(0.5)\n",
    "\n",
    "        ranking2.append(ranking)\n",
    "        title2.append(title.replace(\"\\n\",\"\"))\n",
    "\n",
    "        p_price2.append(p_price.replace(\"\\n\",\"\"))\n",
    "        discount2.append(discount)\n",
    "\n",
    "        try :   \n",
    "            sat_count2.append(sat_count_2)\n",
    "        except IndexError :\n",
    "            sat_count2.append(0)\n",
    "\n",
    "        count += 1\n",
    "    x += 1                \n",
    "    driver.find_element_by_link_text('%s' %x).click() # 다음 페이지번호 클릭\n",
    "          \n",
    "#step 6. csv , xls 형태로 저장하기              \n",
    "co_best_seller = pd.DataFrame()\n",
    "co_best_seller['판매순위']=ranking2\n",
    "co_best_seller['제품소개']=pd.Series(title2)\n",
    "co_best_seller['제품판매가']=pd.Series(p_price2)\n",
    "co_best_seller['할인율']=pd.Series(discount2)\n",
    "co_best_seller['상품평수']=pd.Series(sat_count2)\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "co_best_seller.to_csv(fc_name,encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "# 엑셀 형태로 저장하기\n",
    "co_best_seller.to_excel(fx_name ,index=False)\n",
    "\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "count -= 1\n",
    "print(\"\\n\")\n",
    "print(\"=\" *80)\n",
    "print(\"1.요청된 총 %s 건의 리뷰 중에서 실제 크롤링 된 리뷰수는 %s 건입니다\" %(cnt,count))\n",
    "print(\"2.총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"3.파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"4.파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"5.파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *80)\n",
    "\n",
    "#Step 7. xls 파일에 제품 이미지 삽입하기\n",
    "\n",
    "import win32com.client as win32   #pywin32 , pypiwin32 설치후 동작\n",
    "import win32api                   #파이썬 프롬프트를 관리자 권한으로 실행해야 에러없음\n",
    "                     \n",
    "excel = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "wb = excel.Workbooks.Open(fx_name)\n",
    "sheet = wb.ActiveSheet\n",
    "sheet.Columns(3).ColumnWidth = 30   \n",
    "row_cnt = cnt+1\n",
    "sheet.Rows(\"2:%s\" %row_cnt).RowHeight = 120   \n",
    "\n",
    "ws = wb.Sheets(\"Sheet1\")\n",
    "col_name2=[]\n",
    "file_name2=[]\n",
    "\n",
    "# 이름 불러오기\n",
    "for a in range(2,cnt+2) :\n",
    "      col_name='C'+str(a)\n",
    "      col_name2.append(col_name)\n",
    "      time.sleep(0,3)\n",
    "\n",
    "# 사진 불러오기\n",
    "for b in range(1,cnt+1) :\n",
    "      file_name=img_dir+'\\\\'+str(b)+'.jpg'\n",
    "      file_name2.append(file_name)\n",
    "      time.sleep(0,3)\n",
    "\n",
    "# 합치는 과정    \n",
    "for i in range(0,cnt) :\n",
    "      rng = ws.Range(col_name2[i])\n",
    "      image = ws.Shapes.AddPicture(file_name2[i], False, True, rng.Left, rng.Top, 130, 100)\n",
    "      excel.Visible=True\n",
    "      excel.ActiveWorkbook.Save()\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be929e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
