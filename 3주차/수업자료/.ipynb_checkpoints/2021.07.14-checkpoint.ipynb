{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fd0f22",
   "metadata": {},
   "source": [
    "# Chap 6. 다양한 SNS리뷰 정보 수집하기_인스타그램_유투브 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 인스타 그램의 해쉬태그 수집하기 - by 서진수\n",
    "##########################################################################\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import unicodedata   # 인스타그램의 해시태그 수집 중 자음/모음 분리현상 방지용 모듈\n",
    "\n",
    "#Step 2. 사용자에게 필요한 정보들을를 입력 받습니다.\n",
    "print(\"=\" *70)\n",
    "print(\"   이 크롤러는 인스타그램의 해시태그 정보를 수집합니다\")\n",
    "print(\"   본 제품은 서진수가 교육용으로 특별 제작했으며 \")\n",
    "print(\"   용도외의 사용으로 저작권을 침해하는 행위는 불법입니다\")\n",
    "print(\"   본 제품에 대한 문의는 seojinsu@gmail.com 으로 보내주세요~^^\")\n",
    "print(\"=\" *70)\n",
    "\n",
    "v_id = input(\"1.인스타그램의 ID를 입력하세요: \")\n",
    "v_passwd = input(\"2.인스타그램의 비밀번호를 입력하세요: \")\n",
    "query_txt = input(\"3.검색할 해쉬태그를 입력하세요(예: 강남맛집): \")\n",
    "cnt = int(input('4.크롤링 할 건수는 몇건입니까?: '))\n",
    "real_cnt = math.ceil(cnt / 10)\n",
    "\n",
    "f_dir=input('5.파일이 저장될 경로만 쓰세요(기본경로 : c:\\\\temp\\\\ ) : ')\n",
    "if f_dir =='' :\n",
    "    f_dir = \"c:\\\\temp\\\\\"\n",
    "\n",
    "#Step 3. 결과를 저장할 폴더명과 파일명을 설정하고 폴더를 생성합니다.\n",
    "s_time = time.time( )\n",
    "query_txt2 = '인스타그램'\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "# Step 4. 인스타그램 자동 로그인 하기\n",
    "chrome_path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "\n",
    "driver.get(\"https://www.instagram.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(random.randrange(1,5))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"요청하신 데이터를 추출중이오니 잠시만 기다려 주세요~~~~^^\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#ID와 비번 입력후 로그인하기\n",
    "eid = driver.find_element_by_name('username')\n",
    "for a in v_id :\n",
    "        eid.send_keys(a)\n",
    "        time.sleep(0.3)\n",
    "epwd = driver.find_element_by_name('password')\n",
    "for b in v_passwd :\n",
    "        epwd.send_keys(b)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"loginForm\"]/div/div[3]/button/div').click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5. 검색할 해쉬태그 입력하기\n",
    "#로그인 정보 나중에 저장하기\n",
    "driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/div/div/button').click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_xpath('/html/body/div[4]/div/div/div/div[3]/button[2]').click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 검색할 키워드 입력하기\n",
    "element = driver.find_element_by_xpath('''//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/input''')\n",
    "\n",
    "for c in query_txt :\n",
    "    element.send_keys(c)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/div[3]/div/div[2]/div/div[1]/a').click()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 자동 스크롤다운 함수\n",
    "def scroll_down(driver):\n",
    "  driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "  time.sleep(5)\n",
    "\n",
    "print('요청하신 데이터를 수집중이니 잠시만 기다려 주세요~^^')\n",
    "item=[]     # 인스타그램 URL 주소 저장할 리스트\n",
    "item2=[]\n",
    "    \n",
    "a = 1\n",
    "while (a <= real_cnt):\n",
    "    scroll_down(driver) \n",
    "\n",
    "    # Step 6. 전체 게시물의 원본 URL 추출하기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    all = soup.find('article','KC1QD').find('div','Nnq7C weEfm').find('div','v1Nh3 kIKUG  _bz0w').find_all('a')\n",
    "\n",
    "    for i in all:    \n",
    "        url = i['href']\n",
    "        item.append(url)          \n",
    "        item2 = pd.Series(item).drop_duplicates()\n",
    "    \n",
    "        if len(item2) >= cnt :\n",
    "            break\n",
    "    a += 1\n",
    "    print('요청하신 데이터를 수집중이니 잠시만 더 기다려 주세요~^^')\n",
    "    \n",
    "    if len(item2) >= cnt :\n",
    "        break\n",
    "   \n",
    "# 추출된 URL 사용하여 전체 URL 완성하기\n",
    "full_url=[]\n",
    "url_cnt = 0\n",
    "for x in range(0,len(item2)) :\n",
    "    url = 'https://www.instagram.com' +item[x]\n",
    "    full_url.append(url)\n",
    "    url_cnt += 1\n",
    "    \n",
    "    if url_cnt > cnt:\n",
    "        break\n",
    "\n",
    "# 아래 for 반복문은 수집된 URL 주소를 확인하는 코드임        \n",
    "#for c in range(1,len(full_url)+1) :\n",
    "#    print(c,':',full_url[c-1])\n",
    "\n",
    "#Step 7. 각 페이지별로 그림과 해쉬태그를 수집하기\n",
    "count = 1        # 추출 데이터 건수 세기\n",
    "hash_txt = []    # 해쉬 태그 저장 \n",
    "\n",
    "# 비트맵 이미지 아이콘을 위한 대체 딕셔너리를 만든다\n",
    "import sys\n",
    "bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "count = 0\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt2+'-'+query_txt)\n",
    "f_name=f_dir+s+'-'+query_txt2+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "\n",
    "for c in range(0,len(full_url)) :\n",
    "    driver.get(full_url[c])\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    f = open(f_name, 'a',encoding='UTF-8')\n",
    "\n",
    "    # 해당 페이지의 해시태그 수집\n",
    "    tags = soup.find('div','EtaWk')\n",
    "\n",
    "    try :\n",
    "        tags_1 = tags.find_all('a')\n",
    "    except :\n",
    "        pass\n",
    "    else :\n",
    "        for d in range(0, len(tags_1)) :\n",
    "            tags = tags_1[d].get_text()\n",
    "            tags_11 = tags.translate(bmp_map)\n",
    "            tags_2 = unicodedata.normalize('NFC', tags_11)\n",
    "\n",
    "            for i in tags_2 :\n",
    "                if i[0:1]=='#' :\n",
    "                    hash_txt.append(tags_2)\n",
    "                    print(tags_2)\n",
    "                    f.write(\"\\n\" + str(tags_2))\n",
    "    f.close()\n",
    "    count += 1\n",
    "    \n",
    "#Step 7. 요약 정보 출력하기    \n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"=\" *100)\n",
    "print(\"총 소요시간: %s 초\" %round(t_time,1))\n",
    "print(\"총 저장 건수: %s 건 \" %count)\n",
    "print(\"파일 저장 경로: %s\" %f_name)\n",
    "print(\"=\" *100)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9d0b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서\n",
      "진\n",
      "수\n",
      " \n",
      "꽃\n",
      "미\n",
      "남\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "txt = '서진수 꽃미남'\n",
    "for i in txt:\n",
    "    print(i\n",
    "         )\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e5380",
   "metadata": {},
   "source": [
    "# Chap 7. 다양한 이미지 수집 크롤러 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4734cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사진을 저장할 폴더를 지정하세요(예: c:\\data\\) : c:\\data\\\n",
      "1 번째 이미지 저장중입니다=======\n",
      "2 번째 이미지 저장중입니다=======\n",
      "3 번째 이미지 저장중입니다=======\n",
      "4 번째 이미지 저장중입니다=======\n",
      "5 번째 이미지 저장중입니다=======\n",
      "6 번째 이미지 저장중입니다=======\n",
      "7 번째 이미지 저장중입니다=======\n",
      "8 번째 이미지 저장중입니다=======\n",
      "9 번째 이미지 저장중입니다=======\n",
      "10 번째 이미지 저장중입니다=======\n",
      "11 번째 이미지 저장중입니다=======\n",
      "12 번째 이미지 저장중입니다=======\n",
      "13 번째 이미지 저장중입니다=======\n",
      "14 번째 이미지 저장중입니다=======\n",
      "======================================================================\n",
      "총 소요시간은 17.5 초 입니다 \n",
      "총 저장 건수는 14 건 입니다 \n",
      "파일 저장 경로: c:\\data\\2021-07-14-10-55-25-사진저장 입니다\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#Chap 7-1. 이미지 다운로드용 웹크롤러 만들기\n",
    "# Step 1. 필요한 모듈과 라이브러리를 로딩하고 검색어를 입력 받습니다\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "f_dir = input('사진을 저장할 폴더를 지정하세요(예: c:\\data\\) : ')\n",
    "query_txt ='사진저장'\n",
    "\n",
    "#Step 2. 파일을 저장할 폴더를 생성합니다\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.chdir(f_dir)\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "f_result_dir = f_dir+s+'-'+query_txt\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "path = \"c:/temp/chromedriver_91/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "s_time = time.time( )      # 크롤링 시작 시간을 위한 타임 스탬프를 찍습니다\n",
    "\n",
    "driver.get(\"https://korean.visitkorea.or.kr/detail/rem_detail.html?cotid=be3db10c-b642-409c-81cc-c4cdecb5bd8b&temp=\")\n",
    "time.sleep(2)  # 페이지가 모두 열릴 때 까지 2초 기다립니다.\n",
    "\n",
    "# 학습목표 1: 자동 스크롤다운 함수 만들기\n",
    "# Step 4.  스크롤다운 함수를 생성한 후 실행합니다.\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "scroll_down(driver) \n",
    "    \n",
    "\n",
    "# 본문의 사진 정보를 가져와서 지정된 폴더에 저장하기    \n",
    "# Step 5. 이미지 추출하여 저장하기 \n",
    "\n",
    "file_no = 0                                \n",
    "count = 1\n",
    "img_src2=[]\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img_src = soup.find('div','box_txtPhoto').find_all('img')\n",
    "\n",
    "for i in img_src :\n",
    "        img_src1=i['src']\n",
    "        img_src2.append(img_src1)\n",
    "        count += 1\n",
    "\n",
    "for i in range(0,len(img_src2)) :\n",
    "        try :\n",
    "                urllib.request.urlretrieve(img_src2[i],str(file_no)+'.jpg')\n",
    "        except :\n",
    "                continue        \n",
    "        file_no += 1                \n",
    "        time.sleep(0.5)      \n",
    "        print(\"%s 번째 이미지 저장중입니다=======\" %file_no)\n",
    "       \n",
    "# Step 6. 요약 정보를 출력합니다                \n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "store_cnt = file_no -1\n",
    "\n",
    "print(\"=\" *70)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"총 저장 건수는 %s 건 입니다 \" %file_no)\n",
    "print(\"파일 저장 경로: %s 입니다\" %f_result_dir)\n",
    "print(\"=\" *70)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ab892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
