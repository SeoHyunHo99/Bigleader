{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle [Gender Recognition by Voice]\n",
    "- https://www.kaggle.com/primaryobjects/voicegender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/voice.csv')\n",
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 `train_val_test_split`을 정의: 데이터를 학습 셋 (training set), 검증 셋 (validation set), 테스트 셋 (test set) 3개로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, val_size=0.3, test_size=0.2, random_state=123):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state)\n",
    "    val_size_rev = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                      test_size=val_size_rev,\n",
    "                                                      random_state=random_state)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y,\n",
    "                                                                      val_size=0.3,\n",
    "                                                                      test_size=0.2,\n",
    "                                                                      random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1583, 20)\n",
      "(951, 20)\n",
      "(634, 20)\n"
     ]
    }
   ],
   "source": [
    "# Training 50%, validation 30%, test 20%\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 학습 및 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Decision tree\n",
    "- 나무 깊이에 대해 다른 파라미터값 부여\n",
    "- 각각의 파라미터에 대해 Train set으로 모델을 생성한 후, Validation set으로 성능 평가 ==> 가장 성능이 좋은 파라미터와 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "depth_set = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "dt_models = []\n",
    "accuracy_set = []\n",
    "cm_set = []\n",
    "train_accuracy_set = []\n",
    "\n",
    "for depth in depth_set:\n",
    "    model = DecisionTreeClassifier(max_depth = depth, random_state = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_hat = model.predict(X_train)\n",
    "    y_val_hat = model.predict(X_val)\n",
    "    train_accuracy = metrics.accuracy_score(y_train, \n",
    "                                            y_train_hat)\n",
    "    accuracy = metrics.accuracy_score(y_val, y_val_hat)\n",
    "    cm = metrics.confusion_matrix(y_val, y_val_hat)\n",
    "    \n",
    "    dt_models.append(model)\n",
    "    accuracy_set.append(accuracy)\n",
    "    train_accuracy_set.append(train_accuracy)\n",
    "    cm_set.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.9684542586750788\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 탐색 결과, 가장 좋은 모델과 Validation set에 대한 정확도\n",
    "max_value = max(accuracy_set)\n",
    "max_index = accuracy_set.index(max_value)\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# 가장 좋은 모델\n",
    "best_dt = dt_models[max_index]\n",
    "print(best_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Random Forest\n",
    "- 나무의 갯수, 각 나무의 변수 선택 수를 파라미터로 설정\n",
    "- 각각의 파라미터 집합에 대해 Train set으로 모델을 생성한 후, Validation set으로 성능 평가 ==> 가장 성능이 좋은 파라미터와 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators_set = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "max_features_set = ['auto', 'log2']\n",
    "\n",
    "rf_models = []\n",
    "accuracy_set = []\n",
    "cm_set = []\n",
    "\n",
    "for n_estimators in n_estimators_set:\n",
    "    for max_features in max_features_set:\n",
    "        rf = RandomForestClassifier(n_estimators = n_estimators,\n",
    "                                    max_features = max_features,\n",
    "                                    random_state = 123)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_val_hat = rf.predict(X_val)\n",
    "        accuracy = metrics.accuracy_score(y_val, y_val_hat)\n",
    "        cm = metrics.confusion_matrix(y_val, y_val_hat)\n",
    "\n",
    "        rf_models.append(rf)\n",
    "        accuracy_set.append(accuracy)\n",
    "        cm_set.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.9842271293375394\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 탐색 결과, 가장 좋은 모델과 Validation set에 대한 정확도\n",
    "max_value = max(accuracy_set)\n",
    "max_index = accuracy_set.index(max_value)\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# 가장 좋은 모델\n",
    "best_rf = rf_models[max_index]\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression\n",
    "- 페널티 유형(penalty), 페널티 정도(C), 클래스 가중치 유형(class_weight) 등에 대해 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Hyper-parameter caldidates\n",
    "penalty_set = ['l1', 'l2']\n",
    "C_set = [0.01, 0.1, 1, 10, 100]\n",
    "class_weight_set = [None, 'balanced']\n",
    "\n",
    "# 결과 저장을 미리 할당하기 위한 리스트 선언\n",
    "train_acc_set = []\n",
    "val_acc_set = []\n",
    "lr_models = []\n",
    "\n",
    "for penalty in penalty_set:\n",
    "    for C in C_set:\n",
    "        for class_weight in class_weight_set:\n",
    "            lr = LogisticRegression(penalty=penalty, C=C, \n",
    "                                    class_weight=class_weight,\n",
    "                                    random_state=2072)\n",
    "            # Train the model\n",
    "            lr.fit(X_train, y_train)\n",
    "            lr_models.append(lr)\n",
    "            \n",
    "            # Calculate training accuracy and validation accuracy\n",
    "            y_train_hat = lr.predict(X_train)\n",
    "            y_val_hat = lr.predict(X_val)\n",
    "            train_acc = metrics.accuracy_score(y_train, y_train_hat)\n",
    "            val_acc = metrics.accuracy_score(y_val, y_val_hat)\n",
    "            train_acc_set.append(train_acc)\n",
    "            val_acc_set.append(val_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0.9779179810725552\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 탐색 결과, 가장 좋은 모델과 Validation set에 대한 정확도\n",
    "max_value = max(val_acc_set)\n",
    "max_index = val_acc_set.index(max_value)\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=2072, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# 가장 좋은 모델\n",
    "best_lr = lr_models[max_index]\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. k-Nearest Neighbors Classifier\n",
    "- 이웃 수(n_neighbors), 가중치 부여 방법(weights)을 컨트롤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Hyper-parameter caldidates\n",
    "n_neighbors_set = [10, 15, 20, 25, 30]\n",
    "weights_set = ['uniform', 'distance']\n",
    "\n",
    "# 결과 저장을 미리 할당하기 위한 리스트 선언\n",
    "train_acc_set = []\n",
    "val_acc_set = []\n",
    "knn_models = []\n",
    "\n",
    "for n_neighbors in n_neighbors_set:\n",
    "    for weights in weights_set:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, n_jobs=-1)\n",
    "        \n",
    "        # Train the model\n",
    "        knn.fit(X_train, y_train)\n",
    "        knn_models.append(knn)\n",
    "\n",
    "        # Calculate training accuracy and validation accuracy\n",
    "        y_train_hat = knn.predict(X_train)\n",
    "        y_val_hat = knn.predict(X_val)\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_hat)\n",
    "        val_acc = metrics.accuracy_score(y_val, y_val_hat)\n",
    "        train_acc_set.append(train_acc)\n",
    "        val_acc_set.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.6929547844374343\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 탐색 결과, 가장 좋은 모델과 Validation set에 대한 정확도\n",
    "max_value = max(val_acc_set)\n",
    "max_index = val_acc_set.index(max_value)\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "           weights='distance')\n"
     ]
    }
   ],
   "source": [
    "# 가장 좋은 모델\n",
    "best_knn = knn_models[max_index]\n",
    "print(best_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Naive Bayes Classifier\n",
    "- Naive Bayes Classifier는 딱히 컨트롤할 하이퍼파라미터가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy and validation accuracy\n",
    "y_train_hat = nb.predict(X_train)\n",
    "y_val_hat = nb.predict(X_val)\n",
    "train_acc = metrics.accuracy_score(y_train, y_train_hat)\n",
    "val_acc = metrics.accuracy_score(y_val, y_val_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8633017875920084\n"
     ]
    }
   ],
   "source": [
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n"
     ]
    }
   ],
   "source": [
    "best_nb = nb\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 테스트 셋을 이용하여 가장 성능이 좋은 모델 찾기\n",
    "- 앞서 학습한 Decision Tree, Random Forest, Logistic Regression, k-Nearest Neighbors Classifier, Naive Bayes Classifier 별로 가장 검증 데이터셋에 정확도가 좋았던 모델들을 모은 후\n",
    "- 테스트 셋에 대한 정확도가 가장 좋은 모델을 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
      "            splitter='best'),\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False),\n",
      " LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=2072, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "           weights='distance'),\n",
      " GaussianNB(priors=None)]\n"
     ]
    }
   ],
   "source": [
    "best_models = [best_dt, best_rf, best_lr, best_knn, best_nb]\n",
    "pprint(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
      "            splitter='best')\n",
      "0.9621451104100947\n",
      "[[310  18]\n",
      " [  6 300]]\n",
      "================================================================================\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False)\n",
      "0.9763406940063092\n",
      "[[317  11]\n",
      " [  4 302]]\n",
      "================================================================================\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=2072, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9716088328075709\n",
      "[[314  14]\n",
      " [  4 302]]\n",
      "================================================================================\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n",
      "           weights='distance')\n",
      "0.7050473186119873\n",
      "[[204 124]\n",
      " [ 63 243]]\n",
      "================================================================================\n",
      "GaussianNB(priors=None)\n",
      "0.8927444794952681\n",
      "[[283  45]\n",
      " [ 23 283]]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Trainining set과 Validation set을 합친 후 Test set에 대해 예측 성능 평가\n",
    "X_concat = pd.concat([X_train, X_val])\n",
    "y_concat = pd.concat([y_train, y_val])\n",
    "\n",
    "for best_model in best_models:\n",
    "    \n",
    "    # 합친 데이터에 모델을 refit\n",
    "    best_model.fit(X_concat, y_concat)\n",
    "    \n",
    "    # Test set에 대해 예측 성능 평가\n",
    "    y_test_hat = best_model.predict(X_test)\n",
    "    \n",
    "    print(best_model)\n",
    "    print(metrics.accuracy_score(y_test, y_test_hat))\n",
    "    print(metrics.confusion_matrix(y_test, y_test_hat))\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
